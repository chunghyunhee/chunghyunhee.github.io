---
title : "ML04.Neural Networks : Representation"
toc : True
---

## 1. Motivation


**(1) Non-linear hypotheses**
이미 linear와 logistic이 있는데도 불구하고 neural network가 필요한 이유는 n이 매우 클 경우에 classification문제를 해결해 준다는 점이다. 
only two features일 경우, 단순히 sigmoid function g를 사용한다면 아래와 같이 데이터를 잘 분류할 수 있을 것이다. 

![image](https://user-images.githubusercontent.com/49298791/73378152-7ed3e400-4303-11ea-8276-2f511cedf33c.png)

-하지만 대부분의 경우는 많은 양의 feature가 존재한다. 이 경우 각각의 feature에 대해 좀더 fit하게 하기 위해, 다항식으로 작성하는 콰드라틱 function을 이용하게 된다면 연산의 횟수가 매우 증가하므로 좋지 않아보인다.(앞에서의 logistic과 같은 모델을 사용할 경우에)
-만약 feature을 줄이기 위해 quadratic function만을 도입하고 나머지 parameter을 버리면 hypothesis가 underfit할 수 있고, feature을 cubic까지 도입하면 feature의 수가 많이 가 계산 시간이 매우 많이 걸린다. 따라서 차수를 늘려 해결하는 방법은 좋은 방법이 아니다.

-computer vision에서 car detector을 만든다고 했을 때, label example을 주고 classifier을 진행한다. 특정한 지점을 pixle지점으로 보고, cars와 non-cars를 구분한다고 했을 때, 
![image](https://user-images.githubusercontent.com/49298791/73378194-90b58700-4303-11ea-9a5b-02851f287671.png)

위의 그래프의 경우는 단순히 2개의 feature 픽셀을 나타낸 것이다. 50x50 images에는 n=2500개가 될 것이고, RGB에서는 3 million개의 서로 다른 feature에 대해 classify를 해야 한다. n(large feature을 가진 경우, logistic을 quadratic과 cubic과 같이 다항으로 연결시키는 것은 좋은 방법이 아니다)이 큰 classification에 대해 사용할 수 있는 다른 방법을 생각해 보아야 한다 .


**(2) Neurons and the brain**
위의 문제점에서 눈을 돌려 잠깐 Neurons에 대해 설명한다. 
Neural_Network는 brain을 모방한 것으로, 오래전부터 연구되던 분야이다. 
brain으로 sensor representation이 이루어지고 brain에서는 이 데이터를 어떻게 처리할지에 대해 learning한다. 그 학습하는 과정을 본따 만든 것이 뉴럴 네트워크 학습 알고리즘이다. 


## 2. Neural Networks

**(1) model representation1**

-hypothesis와 model을 어떻게 나타낼 수 있는지. 

-뉴럴 네트워크의 hypothesis를 나타내기 위해서는 single Neuron을 만들어야 한다. cell-body, Dendrite(=input wires), Axon(=output wire)따라서 computation의 형태로 이루어져 있다고 볼 수가 있다. 

-logistic의 형태를 인공적으로 하나의 neuron형태를 그리면 아래와 같다 

![image](https://user-images.githubusercontent.com/49298791/73378280-b5a9fa00-4303-11ea-9561-da2d1d940561.png)

마찬가지로 input wire, body, output wire이 존재하며, 그에 따른 output value에 대한 compute value의 h(x)는 결국 logit함수를 의미한다고 본다. 

-혹은 input으로 bias unit인 x0도 추가할 수 있다. x0은 항상 1이되, 편의에 따라 그리기도 하고 안그리기도 한다. 
-theta를 weights라기도 하며 parameters라고 하기도 한다.


-여러개의 neuron을 결합한 형태를 Neural network라고 한다. 

![image](https://user-images.githubusercontent.com/49298791/73378341-cc505100-4303-11ea-8261-783b0a1097a2.png)

layer1은 input layer, layer3은 output later이다, layer2는 hidden layer이다. 
-결국 computation은 hidden layer에서 일어난다. 
(input layer을 넣었을 때, output이 나올때까지 어떤 일이 있었는지를 알려주는 부분)


-ai^j는 j번째 hidden layer에서 I번째 unit이다. (theta)^j는 layer I 와 layer j+1사이에 적용되는 weight이다. hidden layer는 각 unit마다 weight를 가진다. (다음 layer로 가는 weight라고 생각). outputlayer는 또 hidden layer에서 weight를 받아 완성되는 결괏값이다. 

![image](https://user-images.githubusercontent.com/49298791/73378376-da9e6d00-4303-11ea-94a7-42e2eb1e2d30.png)

-따라서 h(x)는 우리가 알고있는 형태가 된다. 
-즉 각 input을 받아서 weight를 준 형태의 다음 layer을 계산한다. 여기서 weight의 수에 따라 다음 layer의 수가 달라지며, 마지막에는 하나의 output으로 mapping한다. 



**(2) model representation2**
-앞에서 layer값을 input으로 받아 more comoplex feature을 도출하고.의 형태를 통해 최종 h(x)를 계산하는 방법을 forward propagation이라고 한다. 
-superscript에 있는 것은 layer을 의미한다는 점 주의하기.
-각 객체들을 vectorized implementation으로 나타내면 아래와 같은 식으로 h(x)의 변형이 가능하다. (아래는 logistic reg.에 대한 예시)

![image](https://user-images.githubusercontent.com/49298791/73378421-ee49d380-4303-11ea-9345-66084c769b2f.png)

어차피 변수 a의 superscript는 해당 layer의 변수 vector을 의미하므로 달라지는바가 없다. (벡터로 지정해서 정의되는 형태만 달라지는 것)
-결국 g를 logit함수라고 하고, h(x)를 보면 결국 logistc regression의 정의와 동일한 것을 볼 수가 있다. 
-단지 hidden layer에서 x1,x2,x3를 적당한 weight로 훈련하여 새로운 feature a1^(2), a2^(2), a3^(2)를 만들어내고 그걸로 logistic regression을 설명했을 뿐이다. 
-다만, neural network에서는 input값과 theta값에 의해 정해진(=학습된) a^(2)값에 의해 계산된다는 점이다. 따라서 hidden layer을 지나면서 더 complex한 hypothesis를 가정할 수 있게 된다는 것이다. 


-즉 neural network는 각 feature을 훈련시킨다. (원래는 raw값을 넣어 계산했지만, neural network에서의 직관적인 식구성은 같으나, 앞의 변수를 input으로 받아 좀더 complex한 결괏값을 도출한다). 즉 이 과정을 통해 feature을 훈련시텨 다른 값을 가진 feauture을 도출하고, 이 과정을 통해 hypothesis를 고차의 다항식으로 만들지 않고도 n이 매우 큰 경우의 classification을 풀 수 있도록 한다. 



## 3. Applications

**(1) examples and intuitions 1**

-non linear classification을 하는 경우
![image](https://user-images.githubusercontent.com/49298791/73378478-07528480-4304-11ea-9812-370fb22fde4b.png)

-왼쪽의 경우와 오른쪽의 경우는 같은 경우이되, 왼쪽이 classification하기에 더 간단한 형태이다. 이 분류가 오른쪽과 같이 되어있을 때 non-linear fuc의 형태로 분류해보고 싶다. 
- x1 AND x2
layer1에서 layer2로 mapping하는 weight인 theta^(1)을 vectorize하여 임의로 지정했다. 이에 대해 g(.)를 계산하면 아래와 같고 x1,x2는 binary function이므로 이를 적용하여 g function에 매핑하면 분류가 완성된다. 

![image](https://user-images.githubusercontent.com/49298791/73378506-120d1980-4304-11ea-8892-cdaf8d48851c.png)


- OR function
![image](https://user-images.githubusercontent.com/49298791/73378527-1a655480-4304-11ea-95a5-7ca6b79dd3cc.png)



**(2) example and intuitions 2**
즉 원하는 결과에 따라(=알고자 하는 것에 따라) theta값이 달라진다. 즉 optimized theta를 어떻게 찾느냐는 중요한 문제가 될 수도 있다. 


-putting it together : x1 XNOR x2
만약 여러개를 합쳐야 하는 경우에(XNOR)과 같은 경우에, hidden layer을 만들어야 한다. 
앞에서 값을 받아 한번더 complex feature을 만들어 줘야 하기 때문이다. 
(고려해야 할 것이 2개- x1 AND x2 와 (NOT x1) AND (NOT x2)을 계산한 후에 OR)

![image](https://user-images.githubusercontent.com/49298791/73378569-2b15ca80-4304-11ea-9fb5-1c41b58ba4d5.png)

-즉 hidden layer이 반복될수록 “more complex feature”에 대한 도출이 가능하다는 점을 반드시 알아둔다. (각 hidden layer함수를 이용하여 이전 단계의 결과에 대한 어떤 처리를 가해 복잡한 일들을 해내는 것이다)



**(3) multiclass classification**
-neural network를 통해 multiclass classification을 수행할 수 있는 방법
-neural network의 다분류 분제도 regression의 다분류 문제와 동일하게 One-vs-all 문제를 다룬다. 
-computer vision에서 pedestrian, carm motercycle, truck인지 각각에 대해 yes or no를 구분하려고 한다. 즉 각 경우에 대해 원핫인코딩처럼 나오게 될 예정이다. 

![image](https://user-images.githubusercontent.com/49298791/73378640-3cf76d80-4304-11ea-820b-16c229a25d93.png)

